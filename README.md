# -
世界モデルと言語モデルの接地
グループ20
「視覚言語モデルと世界モデルの接地」


視覚言語モデル（VLM）を活用した汎化エージェントの学習は、複数タスクの効率的な解決を目指す分野で重要な課題です。従来の強化学習（RL）は、タスクごとに複雑な報酬設計を必要とし、他タスクへの応用が難しい一方、VLMは自然言語でのタスク指定の容易なインターフェースを提供しますが、応用にあたってはドメイン間ギャップやデータ不足が障害となっていました。本研究では、VLMの表現とRL用生成世界モデルの潜在空間を結びつけます。具体的には、VLMを用いてタスク・観測から得た埋め込み表現を潜在表現に変換し、ロールアウトにおける実際の潜在表現との距離を損失関数として方策を学習します。この手法により、ファインチューニング不要で高い汎用性を実現し、ALFREDベンチマークにおいてマルチモーダルなタスク解決性能の向上ができるかを確かめる。本研究は、生成的世界モデルを活用した汎化エージェント学習の新たな基盤を築くものです。

